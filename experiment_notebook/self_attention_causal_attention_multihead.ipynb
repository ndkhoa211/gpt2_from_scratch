{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Coding Attention Mechanism"
      ],
      "metadata": {
        "id": "BW3wN4mJi7o5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Self-Attention Without Trainable Weights"
      ],
      "metadata": {
        "id": "fEY69Bqy4CnO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a simple example:"
      ],
      "metadata": {
        "id": "1CxYGxrh3dzC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ug_J_HJidIZ",
        "outputId": "a7d22b42-a22a-4364-8f2f-f4fa06d1ad75"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([6, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")\n",
        "\n",
        "inputs.shape # [token, embedding_dim]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_2 = inputs[1] # 2nd input token is the query\n",
        "attention_scores_2 = torch.empty(inputs.shape[0])\n",
        "\n",
        "# compute attention scores\n",
        "for i, input_embedding in enumerate(inputs):\n",
        "  attention_scores_2[i] = torch.dot(query_2, input_embedding)\n",
        "\n",
        "attention_scores_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLnYvh-v3kU8",
        "outputId": "ad492dd2-7d25-4f8e-cc5e-55c266d8fde2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute attention weights\n",
        "attention_weights_2 = torch.softmax(attention_scores_2, dim=0)\n",
        "\n",
        "print(\"attention weights: \", attention_weights_2)\n",
        "print(\"sum:\", attention_weights_2.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oD3Se2hJ4SBy",
        "outputId": "0d813c14-f133-4f55-8e10-67cca4be5fc2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attention weights:  tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
            "sum: tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute context vector\n",
        "context_vector_2 = torch.zeros(inputs.shape[1])\n",
        "\n",
        "for i, input_embedding in enumerate(inputs):\n",
        "  context_vector_2 += attention_weights_2[i] * input_embedding\n",
        "\n",
        "context_vector_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNIHqOdF5-Iv",
        "outputId": "911b6f39-4349-49b0-aec8-72af31365816"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.4419, 0.6515, 0.5683])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compute for all queries simultaneously using matrices:"
      ],
      "metadata": {
        "id": "7p-k7peP7tbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compute attention scores\n",
        "attention_scores = inputs @ inputs.T\n",
        "attention_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JukK-r-f7qo7",
        "outputId": "af4abf71-8db2-4ae9-d9d7-90faac014f2a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
              "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
              "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
              "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
              "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
              "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute attention weights\n",
        "attention_weights = torch.softmax(attention_scores, dim=-1)\n",
        "print(\"row sums:\", attention_weights.sum(dim=-1))\n",
        "attention_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcgW42Fh6in7",
        "outputId": "8f582cf5-91df-4fc3-fc75-0b70ccf36bf6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
              "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
              "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
              "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
              "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
              "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute context vectors\n",
        "context_vectors = attention_weights @ inputs\n",
        "context_vectors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icOWE4Nd9EHS",
        "outputId": "18c6e002-beae-47b7-dffa-ba62869a7c65"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4421, 0.5931, 0.5790],\n",
              "        [0.4419, 0.6515, 0.5683],\n",
              "        [0.4431, 0.6496, 0.5671],\n",
              "        [0.4304, 0.6298, 0.5510],\n",
              "        [0.4671, 0.5910, 0.5266],\n",
              "        [0.4177, 0.6503, 0.5645]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement Self-Attention with Trainable Weights"
      ],
      "metadata": {
        "id": "DHO01f949axm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute KVQ vectors\n"
      ],
      "metadata": {
        "id": "LXBMeitE-fzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define some variables\n",
        "x_2 = inputs[1] # 2nd input token is the query\n",
        "input_embedding_dim = inputs.shape[1] # dim=3\n",
        "output_embedding_dim = 5 # output dim=2"
      ],
      "metadata": {
        "id": "bRd0eUAT9TNs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create KVQ matrices:"
      ],
      "metadata": {
        "id": "KPAW_klv-8Fk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(211)\n",
        "W_query = torch.nn.Parameter(torch.rand(input_embedding_dim, output_embedding_dim),\n",
        "                             requires_grad=False)\n",
        "W_key = torch.nn.Parameter(torch.rand(input_embedding_dim, output_embedding_dim),\n",
        "                           requires_grad=False)\n",
        "W_value = torch.nn.Parameter(torch.rand(input_embedding_dim, output_embedding_dim),\n",
        "                             requires_grad=False)"
      ],
      "metadata": {
        "id": "eGxQrpiX-6IR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# project x_2 into KVQ spaces\n",
        "query_2 = x_2 @ W_query\n",
        "key_2 = x_2 @ W_key\n",
        "value_2 = x_2 @ W_value\n",
        "\n",
        "print(\"query_2:\", query_2)\n",
        "print(\"key_2:\", key_2)\n",
        "print(\"value_2:\", value_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oN1pUVyB_cmv",
        "outputId": "d4b838ce-a384-4c7a-e1bb-c39ca399c619"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query_2: tensor([0.3530, 0.8074, 0.7720, 0.9246, 0.7118])\n",
            "key_2: tensor([0.6885, 0.4568, 1.4283, 1.4383, 1.2989])\n",
            "value_2: tensor([1.2123, 0.6055, 1.0735, 1.3861, 1.5435])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compute all the projected keys and values:"
      ],
      "metadata": {
        "id": "FvmuD6rbAGm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keys = inputs @ W_key\n",
        "values = inputs @ W_value\n",
        "\n",
        "print(\"keys:\", keys)\n",
        "print(\"values:\", values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugizxMq7AGF1",
        "outputId": "5cf09536-3e53-47c8-fb2c-b307f06e99b0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keys: tensor([[0.4207, 0.2279, 1.2477, 0.9477, 0.7168],\n",
            "        [0.6885, 0.4568, 1.4283, 1.4383, 1.2989],\n",
            "        [0.6925, 0.4572, 1.4219, 1.4353, 1.2789],\n",
            "        [0.3541, 0.2496, 0.7168, 0.7615, 0.7592],\n",
            "        [0.5700, 0.3358, 0.9077, 0.9768, 0.5587],\n",
            "        [0.3412, 0.2621, 0.8338, 0.8443, 0.9977]])\n",
            "values: tensor([[0.6973, 0.4836, 1.1020, 1.0985, 0.9538],\n",
            "        [1.2123, 0.6055, 1.0735, 1.3861, 1.5435],\n",
            "        [1.2069, 0.6000, 1.0771, 1.3826, 1.5277],\n",
            "        [0.6661, 0.3190, 0.4886, 0.7061, 0.8630],\n",
            "        [0.7701, 0.3332, 0.8395, 0.9301, 0.8138],\n",
            "        [0.7621, 0.3945, 0.4978, 0.7934, 1.0709]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the attention scores of x_2\n",
        "attention_scores_2 = query_2 @ keys.T\n",
        "attention_scores_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUsvmIFV_4Ss",
        "outputId": "26a4dea8-de93-4218-d936-f53665a8cab9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2.6821, 3.9688, 3.9486, 2.1243, 2.4738, 2.4665])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute attention weights\n",
        "keys_embedding_dim = keys.shape[-1]\n",
        "attention_weights_2 = torch.softmax(attention_scores_2 / keys_embedding_dim**0.5, dim=-1)\n",
        "attention_weights_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cNV7cnIBYTi",
        "outputId": "a550e93b-69ae-41b9-b611-08f1c1270ede"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1401, 0.2491, 0.2468, 0.1092, 0.1276, 0.1272])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute context vectors\n",
        "context_vector_2 = attention_weights_2 @ values\n",
        "context_vector_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1D0yBaICQDL",
        "outputId": "cff36ed4-8d20-4b01-8ca9-45c17913d5a2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.9655, 0.4942, 0.9114, 1.1371, 1.2295])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement a Self-Attention Class"
      ],
      "metadata": {
        "id": "7QXvRnhOCwL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttentionV1(nn.Module):\n",
        "  def __init__(self, input_embedding_dim, output_embedding_dim):\n",
        "    super().__init__()\n",
        "    self.W_query = nn.Parameter(torch.rand(input_embedding_dim, output_embedding_dim))\n",
        "    self.W_key = nn.Parameter(torch.rand(input_embedding_dim, output_embedding_dim))\n",
        "    self.W_value = nn.Parameter(torch.rand(input_embedding_dim, output_embedding_dim))\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    keys = inputs @ self.W_key\n",
        "    values = inputs @ self.W_value\n",
        "    queries = inputs @ self.W_query\n",
        "\n",
        "    attention_scores = queries @ keys.T\n",
        "    attention_weights = torch.softmax(\n",
        "        attention_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "    context_vectors = attention_weights @ values\n",
        "    return context_vectors"
      ],
      "metadata": {
        "id": "JRybDDwICduL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(211)\n",
        "sa_v1 = SelfAttentionV1(input_embedding_dim, output_embedding_dim)\n",
        "context_vectors = sa_v1(inputs)\n",
        "context_vectors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwsYMYeXMVRR",
        "outputId": "358a2dc9-0b4f-45c5-c10e-8ffd4fe3783f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9414, 0.4835, 0.8960, 1.1131, 1.1991],\n",
              "        [0.9655, 0.4942, 0.9114, 1.1371, 1.2295],\n",
              "        [0.9641, 0.4936, 0.9103, 1.1357, 1.2278],\n",
              "        [0.9298, 0.4771, 0.8825, 1.0980, 1.1843],\n",
              "        [0.9173, 0.4713, 0.8726, 1.0845, 1.1688],\n",
              "        [0.9458, 0.4847, 0.8955, 1.1156, 1.2045]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement a Self-Attention Class With Linear Layers"
      ],
      "metadata": {
        "id": "b-gNMn1AMuNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttentionV2(nn.Module):\n",
        "  def __init__(self, input_embedding_dim, output_embedding_dim, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.W_query = nn.Linear(input_embedding_dim, output_embedding_dim,\n",
        "                             bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(input_embedding_dim, output_embedding_dim,\n",
        "                             bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(input_embedding_dim, output_embedding_dim,\n",
        "                             bias=qkv_bias)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    keys = self.W_key(inputs)\n",
        "    values = self.W_value(inputs)\n",
        "    queries = self.W_query(inputs)\n",
        "\n",
        "    attention_scores = queries @ keys.T\n",
        "    attention_weights = torch.softmax(\n",
        "        attention_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "    context_vectors = attention_weights @ values\n",
        "    return context_vectors"
      ],
      "metadata": {
        "id": "-Bj72H8ZMcbg"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(211)\n",
        "sa_v2 = SelfAttentionV2(input_embedding_dim, output_embedding_dim)\n",
        "context_vectors = sa_v2(inputs)\n",
        "context_vectors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOaIC_yxNXA-",
        "outputId": "200d9a6b-2b44-4050-b34a-4684311b4558"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.2783,  0.4747, -0.4724, -0.0341,  0.3074],\n",
              "        [ 0.2768,  0.4746, -0.4740, -0.0356,  0.3078],\n",
              "        [ 0.2768,  0.4747, -0.4741, -0.0353,  0.3078],\n",
              "        [ 0.2798,  0.4775, -0.4724, -0.0288,  0.3086],\n",
              "        [ 0.2779,  0.4780, -0.4733, -0.0269,  0.3090],\n",
              "        [ 0.2799,  0.4764, -0.4724, -0.0316,  0.3081]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Causal Attention Mask"
      ],
      "metadata": {
        "id": "K8WP_-3pObGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setup\n",
        "queries = sa_v2.W_query(inputs)\n",
        "keys = sa_v2.W_key(inputs)\n",
        "attention_scores = queries @ keys.T\n",
        "attention_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2M2C9oZwNZV_",
        "outputId": "811c7dd6-66e2-43c2-a91a-ae579bc3e2cc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1403, -0.0855, -0.1058,  0.0263, -0.4402,  0.2156],\n",
              "        [-0.1708, -0.0352, -0.0575,  0.0688, -0.4432,  0.2793],\n",
              "        [-0.1734, -0.0347, -0.0566,  0.0683, -0.4349,  0.2748],\n",
              "        [-0.0777, -0.0100, -0.0220,  0.0403, -0.2321,  0.1540],\n",
              "        [-0.1706, -0.0153, -0.0234,  0.0407, -0.1629,  0.1161],\n",
              "        [-0.0599, -0.0171, -0.0340,  0.0472, -0.3295,  0.2079]],\n",
              "       grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create the lower triangle matrix of 1's\n",
        "context_length = inputs.shape[0]\n",
        "masked_matrix = torch.triu(torch.ones(context_length, context_length),\n",
        "                           diagonal=1\n",
        "                           )\n",
        "print(masked_matrix)\n",
        "print(masked_matrix.bool())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jw22moHsPRW9",
        "outputId": "a9af9b51-f76d-44ec-c5af-1ecd12e67504"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 1., 1., 1., 1., 1.],\n",
            "        [0., 0., 1., 1., 1., 1.],\n",
            "        [0., 0., 0., 1., 1., 1.],\n",
            "        [0., 0., 0., 0., 1., 1.],\n",
            "        [0., 0., 0., 0., 0., 1.],\n",
            "        [0., 0., 0., 0., 0., 0.]])\n",
            "tensor([[False,  True,  True,  True,  True,  True],\n",
            "        [False, False,  True,  True,  True,  True],\n",
            "        [False, False, False,  True,  True,  True],\n",
            "        [False, False, False, False,  True,  True],\n",
            "        [False, False, False, False, False,  True],\n",
            "        [False, False, False, False, False, False]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# perform masked attention: fill upper triangle of attention_scores with -inf\n",
        "masked_attention_scores = attention_scores.masked_fill(\n",
        "    masked_matrix.bool(), -torch.inf)\n",
        "masked_attention_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lx-j4d0gPuV9",
        "outputId": "57c669ba-1829-417e-afed-231193bdc42b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1403,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
              "        [-0.1708, -0.0352,    -inf,    -inf,    -inf,    -inf],\n",
              "        [-0.1734, -0.0347, -0.0566,    -inf,    -inf,    -inf],\n",
              "        [-0.0777, -0.0100, -0.0220,  0.0403,    -inf,    -inf],\n",
              "        [-0.1706, -0.0153, -0.0234,  0.0407, -0.1629,    -inf],\n",
              "        [-0.0599, -0.0171, -0.0340,  0.0472, -0.3295,  0.2079]],\n",
              "       grad_fn=<MaskedFillBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute masked attention weights\n",
        "masked_attention_weights = torch.softmax(\n",
        "    masked_attention_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "masked_attention_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kORX3A3kP-2p",
        "outputId": "e7d12dfd-3fb8-4845-dc82-9a193cf0f902"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.4848, 0.5152, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3208, 0.3413, 0.3380, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2433, 0.2508, 0.2494, 0.2565, 0.0000, 0.0000],\n",
              "        [0.1907, 0.2045, 0.2037, 0.2097, 0.1914, 0.0000],\n",
              "        [0.1641, 0.1673, 0.1660, 0.1722, 0.1455, 0.1850]],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Masked Attention Weights + Dropout"
      ],
      "metadata": {
        "id": "QK7WgxdlS0Md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(211)\n",
        "dropout_05 = torch.nn.Dropout(p=0.5)\n",
        "dropout_05(masked_attention_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKINtb4JSod-",
        "outputId": "bd6412fe-3901-469f-8fe3-9778db3ad48e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.9697, 1.0303, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.6415, 0.6826, 0.6759, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0000, 0.5016, 0.4989, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3815, 0.4089, 0.4075, 0.4193, 0.0000, 0.0000],\n",
              "        [0.0000, 0.3346, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
              "       grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement a Causual Attention Class"
      ],
      "metadata": {
        "id": "vYklxXgVTZoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# batch setup\n",
        "batch = torch.stack((inputs, inputs), dim=0)\n",
        "batch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jky4e_iBTFkW",
        "outputId": "580e04df-80d2-4ca5-8e0a-66022d5221d2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 6, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalAttention(nn.Module):\n",
        "  def __init__(self,\n",
        "               input_embedding_dim,\n",
        "               output_embedding_dim,\n",
        "               context_length,\n",
        "               dropout,\n",
        "               qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.output_embedding_dim = output_embedding_dim\n",
        "    self.W_query = nn.Linear(input_embedding_dim, output_embedding_dim,\n",
        "                             bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(input_embedding_dim, output_embedding_dim,\n",
        "                             bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(input_embedding_dim, output_embedding_dim,\n",
        "                             bias=qkv_bias)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.register_buffer(\n",
        "        \"mask\",\n",
        "        torch.triu(torch.ones(context_length, context_length),\n",
        "                                            diagonal=1))\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    batch, num_tokens, input_embedding_dim = inputs.shape\n",
        "    keys = self.W_key(inputs)\n",
        "    values = self.W_value(inputs)\n",
        "    queries = self.W_query(inputs)\n",
        "\n",
        "    attention_scores = queries @ keys.transpose(1, 2)\n",
        "    attention_scores.masked_fill_(\n",
        "        self.mask.bool()[:num_tokens, :num_tokens], - torch.inf)\n",
        "    masked_attention_weight = torch.softmax(\n",
        "        attention_scores / (keys.shape[-1]**0.5),\n",
        "        dim=-1)\n",
        "    masked_attention_dropout_weight = self.dropout(masked_attention_weight)\n",
        "\n",
        "    context_vector = masked_attention_dropout_weight @ values\n",
        "    return context_vector"
      ],
      "metadata": {
        "id": "vu82Uxw4T1qS"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create an instance\n",
        "torch.manual_seed(211)\n",
        "context_length = batch.shape[1]\n",
        "ca = CausalAttention(input_embedding_dim=input_embedding_dim,\n",
        "                     output_embedding_dim=output_embedding_dim,\n",
        "                     context_length=context_length,\n",
        "                     dropout=0.2)\n",
        "context_vectors = ca(batch)\n",
        "print(\"context_vector shape: \", context_vectors.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTjmgbV9ir6e",
        "outputId": "fc1f0e9d-c030-47da-ca8f-b03e4a25641e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "context_vector shape:  torch.Size([2, 6, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Head Attention"
      ],
      "metadata": {
        "id": "Dmo8EJwejpUT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a Wrapper Class to Implement Multi-Head Attention (SEQUENTIALLY)"
      ],
      "metadata": {
        "id": "P3IQKOURkqBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionWrapper(nn.Module):\n",
        "  def __init__(self,\n",
        "               input_embedding_dim,\n",
        "               output_embedding_dim,\n",
        "               context_length,\n",
        "               dropout,\n",
        "               num_heads,\n",
        "               qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList(\n",
        "        [CausalAttention(input_embedding_dim,\n",
        "                         output_embedding_dim,\n",
        "                         context_length,\n",
        "                         dropout,\n",
        "                         qkv_bias)\n",
        "            for _ in range(num_heads)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    return torch.cat(\n",
        "        [heads(inputs) for heads in self.heads], # process SEQUENTIALLY\n",
        "        dim=-1)"
      ],
      "metadata": {
        "id": "LH9s883cjY2c"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# variable setup\n",
        "torch.manual_seed(211)\n",
        "context_length = batch.shape[1]\n",
        "input_embedding_dim = 3\n",
        "output_embedding_dim = 5\n",
        "\n",
        "\n",
        "# create an instance\n",
        "mha = MultiHeadAttentionWrapper(\n",
        "    input_embedding_dim,\n",
        "    output_embedding_dim,\n",
        "    context_length,\n",
        "    dropout=0.2,\n",
        "    num_heads=2\n",
        ")\n",
        "context_vectors = mha(batch)\n",
        "\n",
        "print(\"context_vectors shape: \", context_vectors.shape)\n",
        "print(\"context_vectors: \\n\\n\", context_vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltmlPTY0l7s4",
        "outputId": "6c23b411-47e9-4f73-e963-7dbf6f7698a1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "context_vectors shape:  torch.Size([2, 6, 10])\n",
            "context_vectors: \n",
            "\n",
            " tensor([[[ 0.7202,  0.5802, -0.3213,  0.0145,  0.3137,  0.5136,  0.5068,\n",
            "           0.6836,  0.4716, -0.4601],\n",
            "         [ 0.5586,  0.6915, -0.5847, -0.0264,  0.4228,  0.2670,  0.2634,\n",
            "           0.3553,  0.2451, -0.2391],\n",
            "         [ 0.3698,  0.4579, -0.3872, -0.0175,  0.2799,  0.4597,  0.2259,\n",
            "           0.4604,  0.2587, -0.3881],\n",
            "         [ 0.3144,  0.4437, -0.4129, -0.0391,  0.2803,  0.4639,  0.1602,\n",
            "           0.4289,  0.2185, -0.3777],\n",
            "         [ 0.1961,  0.4096, -0.4415, -0.0438,  0.2734,  0.5124,  0.1229,\n",
            "           0.3654,  0.1551, -0.4644],\n",
            "         [ 0.3249,  0.5264, -0.5059, -0.0218,  0.3366,  0.4862,  0.1349,\n",
            "           0.4124,  0.1954, -0.4035]],\n",
            "\n",
            "        [[ 0.7202,  0.5802, -0.3213,  0.0145,  0.3137,  0.5136,  0.5068,\n",
            "           0.6836,  0.4716, -0.4601],\n",
            "         [ 0.5586,  0.6915, -0.5847, -0.0264,  0.4228,  0.6798,  0.3340,\n",
            "           0.6809,  0.3825, -0.5738],\n",
            "         [ 0.3698,  0.4579, -0.3872, -0.0175,  0.2799,  0.4551,  0.2300,\n",
            "           0.4540,  0.2566, -0.3893],\n",
            "         [ 0.4154,  0.6429, -0.6189, -0.0501,  0.4112,  0.6740,  0.1962,\n",
            "           0.5946,  0.2884, -0.5481],\n",
            "         [ 0.3836,  0.6388, -0.5931,  0.0253,  0.4037,  0.2450,  0.0975,\n",
            "           0.1603,  0.0757, -0.2554],\n",
            "         [ 0.3499,  0.5954, -0.5905, -0.0396,  0.3851,  0.5381,  0.1651,\n",
            "           0.4552,  0.2199, -0.4569]]], grad_fn=<CatBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement Multi-Head Attention With Weight Splits (PARALLELLY)"
      ],
      "metadata": {
        "id": "eFMMvEcUnbOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,\n",
        "               input_embedding_dim,\n",
        "               output_embedding_dim,\n",
        "               context_length,\n",
        "               dropout,\n",
        "               num_heads,\n",
        "               qkv_bias=False):\n",
        "    super().__init__()\n",
        "    assert (output_embedding_dim % num_heads == 0), \\\n",
        "          \"output_embedding_dim must be divisible by num_heads\"\n",
        "\n",
        "    self.output_embedding_dim = output_embedding_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = output_embedding_dim // num_heads\n",
        "    self.W_query = nn.Linear(input_embedding_dim, output_embedding_dim,\n",
        "                             bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(input_embedding_dim, output_embedding_dim,\n",
        "                             bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(input_embedding_dim, output_embedding_dim,\n",
        "                             bias=qkv_bias)\n",
        "    self.output_projection = nn.Linear(output_embedding_dim,\n",
        "                                       output_embedding_dim) # to combine head outputs\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.register_buffer(\n",
        "        \"mask\",\n",
        "        torch.triu(torch.ones(context_length, context_length),\n",
        "                                            diagonal=1))\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    batch, num_tokens, input_embedding_dim = inputs.shape\n",
        "\n",
        "    # qkv shapes : (batch, num_tokens, output_embedding_dim)\n",
        "    keys = self.W_key(inputs)\n",
        "    values = self.W_value(inputs)\n",
        "    queries = self.W_query(inputs)\n",
        "\n",
        "    # qkv shapes : (batch, num_tokens, num_heads, head_dim)\n",
        "    keys = keys.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
        "    values = values.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
        "    queries = queries.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "    # qkv shapes : (batch, num_heads, num_tokens, head_dim)\n",
        "    keys = keys.transpose(1, 2)\n",
        "    values = values.transpose(1, 2)\n",
        "    queries = queries.transpose(1, 2)\n",
        "\n",
        "    # compute attention scores for each head\n",
        "    attention_scores = queries @ keys.transpose(3, 2)\n",
        "    attention_scores.masked_fill(\n",
        "        self.mask.bool()[:num_tokens, :num_tokens], - torch.inf)\n",
        "\n",
        "    # compute attention weights + dropout\n",
        "    masked_attention_weight = torch.softmax(\n",
        "        attention_scores / (keys.shape[-1]**0.5),\n",
        "        dim=-1)\n",
        "    masked_attention_dropout_weight = self.dropout(masked_attention_weight)\n",
        "\n",
        "    # compute context vectors\n",
        "    # shape : (batch, num_tokens, num_heads, head_dim)\n",
        "    context_vector = (masked_attention_dropout_weight @ values).transpose(1, 2)\n",
        "\n",
        "    # combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "    # shape : (batch, num_tokens, output_embedding_dim)\n",
        "    context_vector = context_vector.contiguous().view(\n",
        "        batch, num_tokens, self.output_embedding_dim)\n",
        "\n",
        "    # linear projection (optional)\n",
        "    context_vector = self.output_projection(context_vector)\n",
        "\n",
        "    return context_vector"
      ],
      "metadata": {
        "id": "onbmcsi5mqfd"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# variable setup\n",
        "torch.manual_seed(211)\n",
        "print(batch.shape)\n",
        "batch_size, context_length, input_embedding_dim = batch.shape\n",
        "output_embedding_dim = 2\n",
        "\n",
        "\n",
        "# create an instance\n",
        "mha = MultiHeadAttentionWrapper(\n",
        "    input_embedding_dim,\n",
        "    output_embedding_dim,\n",
        "    context_length,\n",
        "    dropout=0.2,\n",
        "    num_heads=3\n",
        ")\n",
        "context_vectors = mha(batch)\n",
        "\n",
        "print(\"context_vectors shape: \", context_vectors.shape)\n",
        "print(\"context_vectors: \\n\\n\", context_vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZC0ppudsI5V",
        "outputId": "a3b79bb1-a582-44f4-cda7-9b66733eb1ef"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 6, 3])\n",
            "context_vectors shape:  torch.Size([2, 6, 6])\n",
            "context_vectors: \n",
            "\n",
            " tensor([[[-0.1217,  0.6112,  0.7202,  0.5802, -0.1637, -0.5053],\n",
            "         [-0.2210,  0.4563,  0.2025,  0.3967, -0.0694, -0.6227],\n",
            "         [-0.0369,  0.1856,  0.5112,  0.7247, -0.0413, -0.6645],\n",
            "         [-0.1377,  0.1109,  0.3178,  0.4444, -0.0060, -0.5926],\n",
            "         [-0.1524,  0.2967,  0.3546,  0.5548, -0.0308, -0.3392],\n",
            "         [-0.0202,  0.1014,  0.2440,  0.3696, -0.0081, -0.4349]],\n",
            "\n",
            "        [[-0.1217,  0.6112,  0.7202,  0.5802, -0.1637, -0.5053],\n",
            "         [-0.2210,  0.4563,  0.5639,  0.6879,  0.0155, -0.3606],\n",
            "         [-0.2433,  0.4075,  0.3766,  0.4592, -0.0413, -0.6645],\n",
            "         [-0.1317,  0.1097,  0.2391,  0.4990, -0.0102, -0.4100],\n",
            "         [-0.1524,  0.2967,  0.3068,  0.4846, -0.0074, -0.4772],\n",
            "         [-0.1034,  0.1737,  0.3131,  0.5049, -0.0053, -0.5535]]],\n",
            "       grad_fn=<CatBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some Note On `.contiguous()`\n",
        "\n",
        "> A tensor is **contiguous** if its data is stored in memory in row-major (C-style) order — meaning the next element in a row is stored next to the current one in memory. Think of it as how the tensor is laid out in memory, not how it looks visually.\n",
        "\n"
      ],
      "metadata": {
        "id": "IhCoX6gi3mtO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Create a 2x3 tensor\n",
        "x = torch.tensor([[1, 2, 3],\n",
        "                  [4, 5, 6]])\n",
        "print(\"Original shape:\", x.shape)\n",
        "print(\"Is contiguous?\", x.is_contiguous())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFrYxJvq3o0w",
        "outputId": "edb9c6e9-8191-42d5-9484-b6312afd05de"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original shape: torch.Size([2, 3])\n",
            "Is contiguous? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transpose it\n",
        "x_t = x.transpose(0, 1)\n",
        "print(x_t)\n",
        "print(\"Transposed shape:\", x_t.shape)\n",
        "print(\"Is contiguous?\", x_t.is_contiguous())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZPCzmkk3r89",
        "outputId": "c3d9e651-05f2-4ad4-8508-b8b1de84ac2b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 4],\n",
            "        [2, 5],\n",
            "        [3, 6]])\n",
            "Transposed shape: torch.Size([3, 2])\n",
            "Is contiguous? False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.stride())      # e.g., (3, 1) → row-major: step 3 for next row\n",
        "print(x_t.stride())    # e.g., (1, 3) → shows it’s not row-major anymore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsv5xTAH7tI6",
        "outputId": "de6b8ebe-5d33-49c2-e033-09dabc51e9a1"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 1)\n",
            "(1, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Try to view it — this will raise an error if you don't call contiguous()\n",
        "try:\n",
        "    x_t.view(-1)\n",
        "except RuntimeError as e:\n",
        "    print(\"Error:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NA0H_kPL3r4l",
        "outputId": "8aae7eda-e028-4ea3-ea87-90a0b9de0c58"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix with .contiguous()\n",
        "x_fixed = x_t.contiguous().view(-1)\n",
        "print(\"Fixed and reshaped:\", x_fixed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hvnevQb3rxA",
        "outputId": "adb4a169-97e8-46c2-9671-0ba3c6cf5b84"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixed and reshaped: tensor([1, 4, 2, 5, 3, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some Note On `output_projection()`\n",
        "\n",
        "This linear layer:\n",
        "\n",
        "- Mixes information across heads\n",
        "\n",
        "- Allows the model to recombine features learned by each head\n",
        "\n",
        "- Helps fuse multiple perspectives (heads) into a single output space\n",
        "\n",
        "Without it:\n",
        "- Each head contributes independently\n",
        "\n",
        "- No opportunity to re-weight or mix features learned by other heads\n",
        "\n",
        "- You would limit the representational power\n",
        "\n"
      ],
      "metadata": {
        "id": "JAIR1yb-2duA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9NZ9-dBltF4A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}