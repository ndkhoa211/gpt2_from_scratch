{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "Mg5Qe7UvWbXw"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "notebook_start_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ],
      "metadata": {
        "id": "zI1PlAAFX0iZ"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pretrain on Unlabled Data"
      ],
      "metadata": {
        "id": "MXE6No3lWw_v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating Generative Text Models"
      ],
      "metadata": {
        "id": "OyJRHCpE9XFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from ch04 import GPT2Model\n",
        "\n",
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,    # Vocabulary size\n",
        "    \"context_length\": 256,  # Shortened context length (orig: 1024)\n",
        "    \"emb_dim\": 768,         # Embedding dimension\n",
        "    \"n_heads\": 12,          # Number of attention heads\n",
        "    \"n_layers\": 12,         # Number of layers\n",
        "    \"drop_rate\": 0.1,       # Dropout rate\n",
        "    \"qkv_bias\": False       # Query-Key-Value bias\n",
        "}\n",
        "torch.manual_seed(211)\n",
        "model = GPT2Model(GPT_CONFIG_124M)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XOTLL9LW3yL",
        "outputId": "5c95888e-9029-420a-803f-12763a0846cf"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2Model(\n",
              "  (token_emb): Embedding(50257, 768)\n",
              "  (position_emb): Embedding(256, 768)\n",
              "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
              "  (transformer_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Utility Functions For Text To Token ID Conversion"
      ],
      "metadata": {
        "id": "tk8umwaXfuEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "from ch04 import generate_text_simple\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "  encoded = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "  # turn the list of token IDs into tensor with batch dimension\n",
        "  encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "  return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(encoded_tensor, tokenizer):\n",
        "  # turn tensor without batch dimension to list\n",
        "  token_ids = encoded_tensor.squeeze(0).tolist()\n",
        "  text = tokenizer.decode(token_ids)\n",
        "  return text"
      ],
      "metadata": {
        "id": "8KHAff--W3vr"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example setup\n",
        "start_context = \"In the midst of winter, I found\"\n",
        "bpe_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "token_ids = generate_text_simple(model=model,\n",
        "                                 input_batch=text_to_token_ids(start_context,\n",
        "                                                               bpe_tokenizer),\n",
        "                                 max_new_tokens=10,\n",
        "                                 context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        "                                 )\n",
        "\n",
        "print(\"output text: \\n\", token_ids_to_text(token_ids, bpe_tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45RaFxH1W3tR",
        "outputId": "c80ba878-d044-476a-e3cb-945668cb7258"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output text: \n",
            " In the midst of winter, I found gluten primarily220 specificity Manufacttions Pictures SiberiaCongress Ess\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate The Text Generation Loss\n",
        "\n",
        "```\n",
        "logits -> probabilities -> target probabilities -> log probabilities -> average log probability -> negative average log probability (i.e. cross entropy loss)\n",
        "```"
      ],
      "metadata": {
        "id": "_LtK-oxVihS1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's experiment on a simple example:"
      ],
      "metadata": {
        "id": "ggsy3zwSjHxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example\n",
        "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
        "                       [40,    1107, 588]])   #  \"I really like\"]\n",
        "\n",
        "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
        "                        [1107,  588, 11311]]) #  \" really like chocolate\"]"
      ],
      "metadata": {
        "id": "7z1lxD6bW3rC"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute logits\n",
        "with torch.no_grad():\n",
        "  logits = model(inputs)\n",
        "\n",
        "# compute probas\n",
        "probas = torch.softmax(logits, dim=-1)\n",
        "print(\"probas shape: \", probas.shape)\n",
        "\n",
        "# compute argmax and retrieve the token ID with highest probas\n",
        "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
        "print(\"token IDs shape: \", token_ids.shape)\n",
        "print(\"token IDs: \\n\", token_ids)\n",
        "\n",
        "# convert token IDs into text\n",
        "print(f\"target batch: {token_ids_to_text(targets[0], bpe_tokenizer)}\")\n",
        "print(f\"predicted batch: {token_ids_to_text(token_ids[0].flatten(), bpe_tokenizer)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbmWF-ZKW3ob",
        "outputId": "6befe6ff-4dc8-49b0-9fbd-a613398fbea6"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probas shape:  torch.Size([2, 3, 50257])\n",
            "token IDs shape:  torch.Size([2, 3, 1])\n",
            "token IDs: \n",
            " tensor([[[26079],\n",
            "         [46561],\n",
            "         [31436]],\n",
            "\n",
            "        [[29764],\n",
            "         [46155],\n",
            "         [45644]]])\n",
            "target batch:  effort moves you\n",
            "predicted batch: ergy digsCVE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieve target token ID probas for each batch\n",
        "text_id = 0\n",
        "target_probas_1 = probas[text_id,\n",
        "                         [0, 1, 2],\n",
        "                         targets[text_id]]\n",
        "print(\"target probas: \", target_probas_1)\n",
        "\n",
        "text_id = 1\n",
        "target_probas_2 = probas[text_id,\n",
        "                         [0, 1, 2],\n",
        "                         targets[text_id]]\n",
        "print(\"target probas: \", target_probas_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJvun0FeW3mA",
        "outputId": "ee3b1828-c935-4218-b8c3-1a8247d8f43e"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "target probas:  tensor([5.8695e-06, 1.1964e-05, 2.3211e-05])\n",
            "target probas:  tensor([1.7885e-05, 1.1387e-05, 1.7034e-05])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute log probabilities\n",
        "log_probas = torch.log(torch.cat([target_probas_1, target_probas_2]))\n",
        "print(\"log probas: \", log_probas)\n",
        "\n",
        "# compute average log probability\n",
        "avg_log_probas = torch.mean(log_probas)\n",
        "print(\"average log probas: \", avg_log_probas)\n",
        "\n",
        "# compute negative average log probability\n",
        "neg_avg_log_probas = avg_log_probas * -1\n",
        "print(\"negative average log probas: \", neg_avg_log_probas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7ZwTx-_W3ji",
        "outputId": "e76fd556-ec3e-49c7-b264-25216ef57ab9"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "log probas:  tensor([-12.0457, -11.3336, -10.6709, -10.9315, -11.3831, -10.9803])\n",
            "average log probas:  tensor(-11.2242)\n",
            "negative average log probas:  tensor(11.2242)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's use pytorch `cross_entropy` function:"
      ],
      "metadata": {
        "id": "-0kQKUyJozDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example setup\n",
        "print(\"logits shape: \", logits.shape)\n",
        "print(\"targets shape: \", targets.shape)\n",
        "\n",
        "# flatten the tensor before plug into cross_entropy\n",
        "logits_flat = logits.flatten(0, 1)\n",
        "targets_flat = targets.flatten()\n",
        "print(\"flatten logits shape: \", logits_flat.shape)\n",
        "print(\"flatten targets shape: \", targets_flat.shape)\n",
        "\n",
        "# compute loss\n",
        "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
        "print(\"loss: \", loss)\n",
        "print(\"loss as float: \", loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ikrjXV0W3hQ",
        "outputId": "b8ae7dd3-7ecc-4a50-c6d0-7f87fbe9524d"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logits shape:  torch.Size([2, 3, 50257])\n",
            "targets shape:  torch.Size([2, 3])\n",
            "flatten logits shape:  torch.Size([6, 50257])\n",
            "flatten targets shape:  torch.Size([6])\n",
            "loss:  tensor(11.2242)\n",
            "loss as float:  11.2241849899292\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating The Training And Validation Set Losses"
      ],
      "metadata": {
        "id": "ZoPMNwDNpje-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setup\n",
        "file_path = \"the-verdict.txt\"\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "  text = f.read()\n",
        "\n",
        "total_characters = len(text)\n",
        "total_tokens = len(bpe_tokenizer.encode(text))\n",
        "print(f\"total characters: {total_characters}\")\n",
        "print(f\"total tokens: {total_tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wvZNR1XW3ew",
        "outputId": "6d78e1a5-bf6e-4885-e3e7-64b9989082f8"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total characters: 20479\n",
            "total tokens: 5145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train-test split\n",
        "train_ratio = 0.90\n",
        "split_id = int(train_ratio * len(text))\n",
        "train_text = text[:split_id]\n",
        "val_text = text[split_id:]"
      ],
      "metadata": {
        "id": "UzbRgEydW3cU"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataloaders\n",
        "from ch02 import create_dataloader_V1\n",
        "torch.manual_seed(211)\n",
        "\n",
        "\n",
        "train_dataloader = create_dataloader_V1(text=train_text,\n",
        "                                        batch_size=2,\n",
        "                                        context_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "                                        stride=GPT_CONFIG_124M['context_length'],\n",
        "                                        shuffle=True,\n",
        "                                        drop_last=True,\n",
        "                                        num_workers=0)\n",
        "\n",
        "val_dataloader = create_dataloader_V1(text=val_text,\n",
        "                                      batch_size=2,\n",
        "                                      context_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "                                      stride=GPT_CONFIG_124M['context_length'],\n",
        "                                      shuffle=False,\n",
        "                                      drop_last=False,\n",
        "                                      num_workers=0)"
      ],
      "metadata": {
        "id": "iTDFK8swW3Zt"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check\n",
        "\n",
        "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
        "    print(\"Not enough tokens for the training loader. \"\n",
        "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
        "          \"increase the `training_ratio`\")\n",
        "\n",
        "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
        "    print(\"Not enough tokens for the validation loader. \"\n",
        "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
        "          \"decrease the `training_ratio`\")"
      ],
      "metadata": {
        "id": "wKzog3NaW3XA"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"train loader:\")\n",
        "for x, y in train_dataloader:\n",
        "  print(x.shape, y.shape)\n",
        "\n",
        "print(\"\\nval loader:\")\n",
        "for x, y in val_dataloader:\n",
        "  print(x.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCJn0rNQW3Ul",
        "outputId": "ea29e6a9-41b3-429a-a4ea-f280c3bb1314"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loader:\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "\n",
            "val loader:\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create an utility function to calculate the cross entropy loss of a given batch returned via the training and validation loader:"
      ],
      "metadata": {
        "id": "eVUhWKt9uPRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_batch(input_batch,\n",
        "                    target_batch,\n",
        "                    model,\n",
        "                    device):\n",
        "  input_batch = input_batch.to(device)\n",
        "  target_batch = target_batch.to(device)\n",
        "\n",
        "  logits = model(input_batch)\n",
        "  loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1),\n",
        "                                           target_batch.flatten())\n",
        "  return loss"
      ],
      "metadata": {
        "id": "mVuvzi8rW3Sb"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we use the above funtion to create a function to compute the training and validation losses:"
      ],
      "metadata": {
        "id": "h8ex3s1pu54s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_loader(dataloader,\n",
        "                     model,\n",
        "                     device,\n",
        "                     num_batches=None):\n",
        "  total_loss = 0.\n",
        "  if len(dataloader) == 0:\n",
        "    return float(\"nan\")\n",
        "  elif num_batches is None:\n",
        "    num_batches = len(dataloader)\n",
        "  else:\n",
        "    # reduce the number of batches to match the total number of batches in the data loader\n",
        "    # if num_batches exceeds the number of batches in the data loader\n",
        "    num_batches = min(num_batches, len(dataloader))\n",
        "  for i, (input_batch, target_batch) in enumerate(dataloader):\n",
        "    if i < num_batches:\n",
        "      loss = calc_loss_batch(input_batch,\n",
        "                             target_batch,\n",
        "                             model,\n",
        "                             device)\n",
        "      total_loss += loss.item()\n",
        "    else:\n",
        "      break\n",
        "  return total_loss / num_batches"
      ],
      "metadata": {
        "id": "xr3Q7qboW3P4"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compute losses!!!"
      ],
      "metadata": {
        "id": "KfiIulECyykj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "compute_loss_start_time = time.time()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "  train_loss = calc_loss_loader(train_dataloader,\n",
        "                                model,\n",
        "                                device)\n",
        "  val_loss = calc_loss_loader(val_dataloader,\n",
        "                              model,\n",
        "                              device)\n",
        "\n",
        "print(f\"train loss: {train_loss:.4f}\")\n",
        "print(f\"validation loss: {val_loss:.4f}\")\n",
        "\n",
        "\n",
        "compute_loss_end_time = time.time()\n",
        "runtime_in_seconds = compute_loss_end_time - compute_loss_start_time\n",
        "# format as minutes and seconds\n",
        "minutes, seconds = divmod(runtime_in_seconds, 60)\n",
        "print(f\"compute_loss runtime: {int(minutes)} min {seconds:.2f} sec\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_4klJ6_W3Ni",
        "outputId": "33567d89-7c84-4971-bfba-16a07c29186f"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 10.9759\n",
            "validation loss: 11.0311\n",
            "compute_loss runtime: 0 min 0.73 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training An LLM"
      ],
      "metadata": {
        "id": "XXLpa82X0BrE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's implement a training function:"
      ],
      "metadata": {
        "id": "Rp60IL3D1Sic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_simple(model,\n",
        "                       train_loader,\n",
        "                       val_loader,\n",
        "                       optimizer,\n",
        "                       device,\n",
        "                       num_epochs,\n",
        "                       eval_freq,\n",
        "                       eval_iter,\n",
        "                       start_context,\n",
        "                       tokenizer):\n",
        "\n",
        "  # initialize lists to track losses and tokens seen\n",
        "  train_losses = []\n",
        "  val_losses = []\n",
        "  track_tokens_seen = []\n",
        "  token_seen = 0\n",
        "  global_step = -1\n",
        "\n",
        "  # main training loop - iterate over training epochs\n",
        "  for epoch in range(num_epochs):\n",
        "    # set model to training mode\n",
        "    model.train()\n",
        "\n",
        "    # iterate over batches in each training epoch\n",
        "    for input_batch, target_batch in train_loader:\n",
        "      # reset loss gradients from previous batch iteration\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # calculate loss on current batch\n",
        "      loss = calc_loss_batch(input_batch,\n",
        "                             target_batch,\n",
        "                             model,\n",
        "                             device)\n",
        "\n",
        "      # backward pass to calculate loss gradients\n",
        "      loss.backward()\n",
        "\n",
        "      # update model weights using loss gradients\n",
        "      optimizer.step()\n",
        "      token_seen += input_batch.numel()\n",
        "      global_step += 1\n",
        "\n",
        "      # optional evaluation step\n",
        "      if global_step % eval_freq == 0:\n",
        "        train_loss, val_loss = evaluate_model(model,\n",
        "                                              train_loader,\n",
        "                                              val_loader,\n",
        "                                              device,\n",
        "                                              eval_iter)\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        track_tokens_seen.append(token_seen)\n",
        "        # print training and evaluation set loss\n",
        "        print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "              f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "    # generative sample text for visual inspection\n",
        "    generate_and_print_sample(model,\n",
        "                              tokenizer,\n",
        "                              device,\n",
        "                              start_context)\n",
        "\n",
        "  return train_losses, val_losses, track_tokens_seen\n",
        "\n",
        "\n",
        "def evaluate_model(model,\n",
        "                    train_loader,\n",
        "                    val_loader,\n",
        "                    device,\n",
        "                    eval_iter):\n",
        "  # set model to evaluation mode\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    # calculate loss\n",
        "    train_loss = calc_loss_loader(train_loader,\n",
        "                                  model,\n",
        "                                  device,\n",
        "                                  num_batches=eval_iter)\n",
        "    val_loss = calc_loss_loader(val_loader,\n",
        "                                model,\n",
        "                                device,\n",
        "                                num_batches=eval_iter)\n",
        "\n",
        "  # set model bacl to training mode\n",
        "  model.train()\n",
        "  return train_loss, val_loss\n",
        "\n",
        "\n",
        "def generate_and_print_sample(model,\n",
        "                              tokenizer,\n",
        "                              device,\n",
        "                              start_context):\n",
        "  # set model to evaluation mode\n",
        "  model.eval()\n",
        "  context_size = model.position_emb.weight.shape[0]\n",
        "  encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "  with torch.no_grad():\n",
        "    token_ids = generate_text_simple(model=model,\n",
        "                                     input_batch=encoded,\n",
        "                                     max_new_tokens=50,\n",
        "                                     context_size=context_size)\n",
        "  decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "  print(decoded_text.replace(\"\\n\", \" \")) # compact print format\n",
        "  # set model bacl to training mode\n",
        "  model.train()"
      ],
      "metadata": {
        "id": "18JgciImW3K5"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's train the LLM:"
      ],
      "metadata": {
        "id": "coJDLqIc6FFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "training_start_time = time.time()\n",
        "\n",
        "torch.manual_seed(211)\n",
        "model = GPT2Model(GPT_CONFIG_124M).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(),\n",
        "                              lr=0.0004,\n",
        "                              weight_decay=0.1)\n",
        "num_epochs = 20\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(model=model,\n",
        "                                                             train_loader=train_dataloader,\n",
        "                                                             val_loader=val_dataloader,\n",
        "                                                             optimizer=optimizer,\n",
        "                                                             device=device,\n",
        "                                                             num_epochs=num_epochs,\n",
        "                                                             eval_freq=5,\n",
        "                                                             eval_iter=5,\n",
        "                                                             start_context=\"In the midst of winter, I found\",\n",
        "                                                             tokenizer=bpe_tokenizer)\n",
        "\n",
        "\n",
        "training_end_time = time.time()\n",
        "runtime_in_seconds = training_end_time - training_start_time\n",
        "# format as minutes and seconds\n",
        "minutes, seconds = divmod(runtime_in_seconds, 60)\n",
        "print(f\"device: {device}\")\n",
        "print(f\"training runtime: {int(minutes)} min {seconds:.2f} sec\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2Vl0vO3W3I6",
        "outputId": "7c42671e-3ff0-4ca4-cb58-e01bb7a316b2"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000): Train loss 9.793, Val loss 10.015\n",
            "Ep 1 (Step 000005): Train loss 8.116, Val loss 8.308\n",
            "In the midst of winter, I found the                                                 \n",
            "Ep 2 (Step 000010): Train loss 6.760, Val loss 7.085\n",
            "Ep 2 (Step 000015): Train loss 6.054, Val loss 6.605\n",
            "In the midst of winter, I found the, and, and, the, the, and, and, and, the, and, the the the the, the, and the, and, the the the, the the the the, the the the the the, the, the\n",
            "Ep 3 (Step 000020): Train loss 7.540, Val loss 9.204\n",
            "Ep 3 (Step 000025): Train loss 5.570, Val loss 6.454\n",
            "In the midst of winter, I found, and, and I was, and the.                                        \n",
            "Ep 4 (Step 000030): Train loss 5.501, Val loss 6.621\n",
            "Ep 4 (Step 000035): Train loss 5.269, Val loss 6.500\n",
            "In the midst of winter, I found the with the I had I had \" had the I had the I had \" I had the I had the with the and I had the with the with the with the with the I had the and I had the he was to the with the with\n",
            "Ep 5 (Step 000040): Train loss 4.665, Val loss 6.396\n",
            "In the midst of winter, I found, and, and I had been a little.                                        \n",
            "Ep 6 (Step 000045): Train loss 4.258, Val loss 6.330\n",
            "Ep 6 (Step 000050): Train loss 3.921, Val loss 6.350\n",
            "In the midst of winter, I found-rooms, I said--I have the picture--the and I felt him, I felt, I felt to see his pictures, and I had been the picture--his I had been his pictures--and it, I felt to see it,\n",
            "Ep 7 (Step 000055): Train loss 3.282, Val loss 6.238\n",
            "Ep 7 (Step 000060): Train loss 2.656, Val loss 6.290\n",
            "In the midst of winter, I found-rooms, I had been his pictures. Gisburn. Gisburn's an!     \"I didn't you know; and I had been the fact of his pictures--I didn't--as it, I had\n",
            "Ep 8 (Step 000065): Train loss 2.243, Val loss 6.284\n",
            "Ep 8 (Step 000070): Train loss 1.736, Val loss 6.264\n",
            "In the midst of winter, I found--and that he was him, I had the last word. Gisburn's an awful simple's \"Yes--his just he had the house.\"    \"I had been the man of the hour. The younger artist was said\n",
            "Ep 9 (Step 000075): Train loss 1.550, Val loss 6.306\n",
            "Ep 9 (Step 000080): Train loss 1.186, Val loss 6.408\n",
            "In the midst of winter, I found me--I glanced after him, a good fellow--and. Gisburn's past!      \"Oh, the window-curtains, as I had been the man of the hour. The younger artist was said\n",
            "Ep 10 (Step 000085): Train loss 0.909, Val loss 6.443\n",
            "In the midst of winter, I found me, for he was \"interesting\": on that point I could have given Miss Croft the fact, becoming the man of the moment--as Jack himself, one might put it, had been the man of the hour. The younger artist was said\n",
            "Ep 11 (Step 000090): Train loss 0.675, Val loss 6.442\n",
            "Ep 11 (Step 000095): Train loss 0.449, Val loss 6.482\n",
            "In the midst of winter, I found me--quite insensible to the irony. Gisburn's: make yourself comfortable--and here are the cigars you like.\"                        \n",
            "Ep 12 (Step 000100): Train loss 0.354, Val loss 6.577\n",
            "Ep 12 (Step 000105): Train loss 0.316, Val loss 6.696\n",
            "In the midst of winter, I found me, for he answered with a deprecating laugh: \"Yes--and by me!\"  \"I what a degree he had the window-curtains, moved aside a _jardiniere_ full of pink azaleas\n",
            "Ep 13 (Step 000110): Train loss 0.235, Val loss 6.744\n",
            "Ep 13 (Step 000115): Train loss 0.179, Val loss 6.809\n",
            "In the midst of winter, I found me, for he answered with a deprecating laugh: \"Yes--she's an awful simpleton, you know, and threw back the same quality as his pictures--since he liked his ease--because he didn't want to go on painting\n",
            "Ep 14 (Step 000120): Train loss 0.171, Val loss 6.838\n",
            "Ep 14 (Step 000125): Train loss 0.146, Val loss 6.907\n",
            "In the midst of winter, I found me I felt able to face the irony. She wanted him vindicated--and by me!\"  He laughed again, and, on a later day, one might put it, married a rich widow, and established himself in a villa on\n",
            "Ep 15 (Step 000130): Train loss 0.134, Val loss 6.940\n",
            "In the midst of winter, I found me brush.\"  And his tone told me in a flash that he never thought of anything else.  I moved away, instinctively embarrassed by my unexpected discovery; and as I turned, my eye fell on a small picture above the mantel\n",
            "Ep 16 (Step 000135): Train loss 0.120, Val loss 6.993\n",
            "Ep 16 (Step 000140): Train loss 0.113, Val loss 7.059\n",
            "In the midst of winter, I found me still a cheap genius--I told Mrs. Stroud so when she began to stammer something about her poverty. I remember getting off a prodigious phrase about the honour being _mine_--oh, I was princely, beaming\n",
            "Ep 17 (Step 000145): Train loss 0.090, Val loss 7.085\n",
            "Ep 17 (Step 000150): Train loss 0.109, Val loss 7.084\n",
            "In the midst of winter, I found me I felt able to face the irony. She wanted him vindicated--and by me!\" I was hear that, in the height of his head to look up at the honour being _mine_--oh, I was princely, my dear\n",
            "Ep 18 (Step 000155): Train loss 0.068, Val loss 7.127\n",
            "Ep 18 (Step 000160): Train loss 0.049, Val loss 7.102\n",
            "In the midst of winter, I found me, for he answered with a deprecating laugh: \"Yes--she's past!  Mrs. It was just because she was _not_ interesting--if I may be pardoned the bull--ah, poor Stroud! She\n",
            "Ep 19 (Step 000165): Train loss 0.059, Val loss 7.099\n",
            "Ep 19 (Step 000170): Train loss 0.051, Val loss 7.152\n",
            "In the midst of winter, I found--forming, as it were, struck by his last word. Poor Jack Gisburn! The women had made him--it was fitting that they should mourn him. Among his own sex fewer regrets were heard, and in his own trade hardly a\n",
            "Ep 20 (Step 000175): Train loss 0.049, Val loss 7.136\n",
            "In the midst of winter, I found-stream stroke. Gisburn--as such--had not existed till nearly a year after Jack's resolve had been taken. It might be that he had married her--since he liked his ease--because he didn't want to go on painting\n",
            "device: cuda\n",
            "training runtime: 1 min 0.29 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot a loss graph:"
      ],
      "metadata": {
        "id": "q3hEb6gFACj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "\n",
        "def plot_losses(epoch_seen,\n",
        "                tokens_seen,\n",
        "                train_losses,\n",
        "                val_losses):\n",
        "  fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "  # plot training and validation loss against epochs\n",
        "  ax1.plot(epoch_seen, train_losses, label=\"Training Loss\")\n",
        "  ax1.plot(epoch_seen, val_losses, linestyle=\"-.\", label=\"Validation Loss\")\n",
        "  ax1.set_xlabel(\"Epoch\")\n",
        "  ax1.set_ylabel(\"Loss\")\n",
        "  ax1.legend(loc=\"upper right\")\n",
        "  ax1.xaxis.set_major_locator(MaxNLocator(integer=True)) # only show integer labels on x-axis\n",
        "\n",
        "  # create a second x-axis for token seen\n",
        "  ax2 = ax1.twiny() # create a second x-axis that shares the same y-axis\n",
        "  ax2.plot(tokens_seen, train_losses, alpha=0) # invisible plot for aligning ticks\n",
        "  ax2.set_xlabel(\"Tokens Seen\")\n",
        "\n",
        "  fig.tight_layout() # asjust layput to make room\n",
        "  plt.savefig(\"loss_plot.pdf\")\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "E-h7BHpEW3GZ",
        "outputId": "c2dc8eb4-2ce2-47a5-abc9-76c8cf280d6d"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoding Strategies To Control Randomness"
      ],
      "metadata": {
        "id": "zT4B20o6Cih_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(\"cpu\")\n",
        "model.eval()\n",
        "\n",
        "bpe_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "token_ids = generate_text_simple(model=model,\n",
        "                                 input_batch=text_to_token_ids(\"In the midst of winter, I found\",\n",
        "                                                               bpe_tokenizer),\n",
        "                                 max_new_tokens=20,\n",
        "                                 context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        "                                 )\n",
        "\n",
        "print(\"output text: \\n\", token_ids_to_text(token_ids, bpe_tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBQnMpP1W3EO",
        "outputId": "3e30488b-4f70-4632-8e1f-04b30e86c780"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output text: \n",
            " In the midst of winter, I found-stream stroke. Gisburn--as such--had not existed till nearly a year after Jack\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Temperature Scaling"
      ],
      "metadata": {
        "id": "-z-E9zPJGwqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example setup\n",
        "vocab = {\n",
        "    \"closer\": 0,\n",
        "    \"every\": 1,\n",
        "    \"effort\": 2,\n",
        "    \"forward\": 3,\n",
        "    \"inches\": 4,\n",
        "    \"moves\": 5,\n",
        "    \"pizza\": 6,\n",
        "    \"toward\": 7,\n",
        "    \"you\": 8,\n",
        "}\n",
        "\n",
        "inverse_vocab = {v: k for k, v in vocab.items()}\n",
        "\n",
        "# Suppose input is \"every effort moves you\", and the LLM\n",
        "# returns the following logits for the next token:\n",
        "next_token_logits = torch.tensor(\n",
        "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
        ")\n",
        "\n",
        "probas = torch.softmax(next_token_logits, dim=0)\n",
        "next_token_id = torch.argmax(probas).item()\n",
        "\n",
        "# The next generated token is then as follows:\n",
        "print(inverse_vocab[next_token_id])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8l-cdD0W3B2",
        "outputId": "69cc70b3-6365-4db9-bd22-9019aa7bee31"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "forward\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(211)\n",
        "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
        "print(inverse_vocab[next_token_id])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7y2fU0IW2__",
        "outputId": "8e9a9483-8ba8-4523-a774-28e841153129"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "forward\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_sampled_tokens(probas):\n",
        "    torch.manual_seed(211) # Manual seed for reproducibility\n",
        "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
        "    sampled_ids = torch.bincount(torch.tensor(sample), minlength=len(probas))\n",
        "    for i, freq in enumerate(sampled_ids):\n",
        "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
        "\n",
        "print_sampled_tokens(probas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZCyEBuvWcwX",
        "outputId": "28817195-1d45-4018-9e25-1f61fb7092e2"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "75 x closer\n",
            "2 x every\n",
            "0 x effort\n",
            "568 x forward\n",
            "3 x inches\n",
            "2 x moves\n",
            "0 x pizza\n",
            "350 x toward\n",
            "0 x you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_with_temperature(logits, temperature):\n",
        "    scaled_logits = logits / temperature\n",
        "    return torch.softmax(scaled_logits, dim=0)\n",
        "\n",
        "# Temperature values\n",
        "temperatures = [1, 0.1, 5]  # Original, higher confidence, and lower confidence\n",
        "\n",
        "# Calculate scaled probabilities\n",
        "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]"
      ],
      "metadata": {
        "id": "FwDdXVoWWcuS"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting\n",
        "x = torch.arange(len(vocab))\n",
        "bar_width = 0.15\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5, 3))\n",
        "for i, T in enumerate(temperatures):\n",
        "    rects = ax.bar(x + i * bar_width, scaled_probas[i], bar_width, label=f'Temperature = {T}')\n",
        "\n",
        "ax.set_ylabel('Probability')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
        "ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"temperature-plot.pdf\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "CfNIf-HUWcsD",
        "outputId": "41b7fc74-87d7-4add-b81d-9d5b4d400618"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print_sampled_tokens(scaled_probas[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSLAhk0DWcp6",
        "outputId": "a76fb9c5-5b30-41bf-fa4f-e9c87761d5e0"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 x closer\n",
            "0 x every\n",
            "0 x effort\n",
            "987 x forward\n",
            "0 x inches\n",
            "0 x moves\n",
            "0 x pizza\n",
            "13 x toward\n",
            "0 x you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_sampled_tokens(scaled_probas[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsHWnRl5Wcnx",
        "outputId": "f86da466-3174-4835-8017-1330cbd7004f"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "158 x closer\n",
            "76 x every\n",
            "42 x effort\n",
            "243 x forward\n",
            "86 x inches\n",
            "52 x moves\n",
            "40 x pizza\n",
            "228 x toward\n",
            "75 x you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Top-k Sampling"
      ],
      "metadata": {
        "id": "p2MymI_hInEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_k = 3\n",
        "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
        "\n",
        "print(\"Top logits:\", top_logits)\n",
        "print(\"Top positions:\", top_pos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKjK21vaWcig",
        "outputId": "19a28890-dfad-41c9-9e14-8146ca4ea821"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
            "Top positions: tensor([3, 7, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_logits = torch.where(\n",
        "    condition=next_token_logits < top_logits[-1],\n",
        "    input=torch.tensor(float(\"-inf\")),\n",
        "    other=next_token_logits\n",
        ")\n",
        "\n",
        "print(new_logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rueIQpVXImvg",
        "outputId": "e4fd0102-552d-4b3a-d1b3-81d4681d5265"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "see https://github.com/rasbt/LLMs-from-scratch/discussions/326"
      ],
      "metadata": {
        "id": "AacnTHGUKD_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topk_probas = torch.softmax(new_logits, dim=0)\n",
        "print(topk_probas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWU8PkhAImr-",
        "outputId": "8e0cf013-4640-4591-cdfc-71c7806683b9"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modifying The Text Generation Function"
      ],
      "metadata": {
        "id": "jyeKdAahKKDR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's add `temperature sampling` and `top-k` sampling to the `generate_text_simple()` function from `ch04.py`:"
      ],
      "metadata": {
        "id": "El684OA1KXHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model,\n",
        "                  input_batch,\n",
        "                  max_new_tokens,\n",
        "                  context_size,\n",
        "                  temperature=0.0,\n",
        "                  top_k=None,\n",
        "                  eos_id=None):\n",
        "  for _ in range(max_new_tokens):\n",
        "    # crop current context if it exceeds the supported context_size\n",
        "    crop_input_batch = input_batch[:, -context_size:]\n",
        "\n",
        "    # predict next token\n",
        "    with torch.no_grad():\n",
        "      logits = model(crop_input_batch)\n",
        "\n",
        "    # consider only logits of the last token\n",
        "    logits = logits[:, -1, :] # (batch, n_tokens, vocab_size) -> (batch, vocab_size)\n",
        "\n",
        "    # NEW: filter logits with top_k sampling\n",
        "    if top_k is not None:\n",
        "      # keep only top_k values\n",
        "      top_logits, _ = torch.topk(logits, top_k)\n",
        "      min_val = top_logits[:, -1] # min value among the top_k values\n",
        "      # all values other than top_k values will be set to -inf\n",
        "      logits = torch.where(logits < min_val,\n",
        "                           torch.tensor(-torch.inf).to(logits.device),\n",
        "                           logits)\n",
        "\n",
        "    # NEW: temperature scaling\n",
        "    if temperature > 0.0:\n",
        "      logits = logits / temperature\n",
        "\n",
        "      probas = torch.softmax(logits, dim=-1) # (batch, vocab_size)\n",
        "      predicted_tokens = torch.multinomial(probas, num_samples=1) # (batch, 1)\n",
        "\n",
        "    else: # same as before\n",
        "      probas = torch.softmax(logits, dim=-1) # (batch, vocab_size)\n",
        "      predicted_tokens = torch.argmax(probas, dim=-1, keepdim=True) # (batch, 1)\n",
        "\n",
        "    if predicted_tokens == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
        "            break\n",
        "\n",
        "    # update input_batch (append predicted tokens to the sequences)\n",
        "    input_batch = torch.cat([input_batch, predicted_tokens], dim=-1) # [batch, num_tokens+1]\n",
        "\n",
        "  return input_batch"
      ],
      "metadata": {
        "id": "qd1aira_Impc"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test the function\n",
        "torch.manual_seed(211)\n",
        "token_ids = generate_text(model=model,\n",
        "                          input_batch=text_to_token_ids(\"In the midst of winter, I found\",\n",
        "                                                        bpe_tokenizer),\n",
        "                          max_new_tokens=20,\n",
        "                          context_size=GPT_CONFIG_124M[\"context_length\"],\n",
        "                          temperature=1.4,\n",
        "                          top_k=25\n",
        "                          )\n",
        "\n",
        "print(\"output text: \\n\", token_ids_to_text(token_ids, bpe_tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J14oxtVuImm_",
        "outputId": "3c0cea54-64e1-4851-a239-e81228d66ee7"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output text: \n",
            " In the midst of winter, I found of the axioms he laid down, tips of a self-confident m past! The\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load And Save Model Weights In Pytorch"
      ],
      "metadata": {
        "id": "n5DEao0qOZMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save model state_dict\n",
        "torch.save(model.state_dict(), \"model.pth\")"
      ],
      "metadata": {
        "id": "bPd3mWdYImkl"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load model state_dict\n",
        "model = GPT2Model(GPT_CONFIG_124M)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.load_state_dict(torch.load(\"model.pth\", map_location=device, weights_only=True))\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJaqH1o4ImiN",
        "outputId": "20ef314f-02e3-46a8-8d36-38fb0299982a"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2Model(\n",
              "  (token_emb): Embedding(50257, 768)\n",
              "  (position_emb): Embedding(256, 768)\n",
              "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
              "  (transformer_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can save and load model together with the optimizers' additional parameters:"
      ],
      "metadata": {
        "id": "5NfGIy7mPn6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save({\n",
        "    \"model_state_dict\": model.state_dict(),\n",
        "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "    },\n",
        "    \"model_and_optimizer.pth\"\n",
        ")"
      ],
      "metadata": {
        "id": "fet0eJPEImf5"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load(\"model_and_optimizer.pth\", weights_only=True)\n",
        "\n",
        "model = GPT2Model(GPT_CONFIG_124M)\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.1)\n",
        "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hK7MSCx7ImdQ",
        "outputId": "9aab5bf3-afdb-4a11-ce1e-56ad5222cd80"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2Model(\n",
              "  (token_emb): Embedding(50257, 768)\n",
              "  (position_emb): Embedding(256, 768)\n",
              "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
              "  (transformer_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nJVYh900Imax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KZg85k1JImYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LygVVpixImQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_end_time = time.time()\n",
        "runtime_in_seconds = notebook_end_time - notebook_start_time\n",
        "\n",
        "# format as minutes and seconds\n",
        "minutes, seconds = divmod(runtime_in_seconds, 60)\n",
        "print(f\"Notebook runtime: {int(minutes)} min {seconds:.2f} sec\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUEJEePKWcfs",
        "outputId": "982546f0-4e67-42e7-d606-eed293be2339"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Notebook runtime: 4 min 58.12 sec\n"
          ]
        }
      ]
    }
  ]
}